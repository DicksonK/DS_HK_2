{
 "metadata": {
  "name": "",
  "signature": "sha256:3a19b76ac226b5949b7c387c7863e17a5a66834470bda0e4e325daaccc4f1641"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%javascript\n",
      "function is_local(){\n",
      "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
      "}\n",
      "var url = is_local() ? \"http://localhost:8000/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
      "$.getScript(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Big Data: StarCluster, Hadoop & MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Not everything that can be counted counts, and not everything that counts can be counted.\n",
      "\n",
      "<footer>~ William Bruce Cameron</footer>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/agenda.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Big Data\n",
      "2. Amazon Web Services\n",
      "3. StarCluster\n",
      "4. MapReduce Programming Model\n",
      "5. Implementation Details\n",
      "6. Word Count Example\n",
      "\n",
      "**Labs**\n",
      "1. Setting up a cluster on AWS\n",
      "2. Hadoop Streaming"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Big Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What does \u201cbig data\u201d actually refer to? A common approach is to talk about _Big data_ in terms of the 3 V's commonly associated with it."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-   Volume\n",
      "-   Velocity\n",
      "-   Variety"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The 3 Vs of Big Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Volume (i.e. more than can fit in memory on your laptop)  \n",
      "    - e.g. Amazon\u2019s user behavior data\n",
      "\n",
      "\n",
      "- Velocity (i.e. faster than standard machines can process)\n",
      "    - e.g. Twitter\u2019s \u201cFirehose\u201d of tweets\n",
      "\n",
      "\n",
      "- Variety (i.e. does not conform to a single structure)\n",
      "    - e.g. Google\u2019s cache of web pages"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this course, we will not deal with velocity. That is really more of an engineering problem than a data science problem. In fact, all three of these are engineering problems, but only the first and the third are really germane to Data Science. That said, as a Data Scientist, it is probably only a matter of time before someone asks you to create a real-time dashboard. Such a thing, while cool, is often very difficult to create and has questionable utility for anyone besides an operations engineer or a day trader, and they already have real-time dashboards."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Meeting the needs of Big Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How would you approach dealing with this kind of data? One approach would be to get a huge supercomputer. But this has some obvious drawbacks: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- expensive\n",
      "- difficult to maintain\n",
      "- scalability is bounded"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead of one huge machine, what if we got a bunch of regular (*commodity*) machines? This has obvious benefits!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- cheaper\n",
      "- easier to maintain\n",
      "- scalability is unbounded (just add more nodes to the *cluster*)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can visualize this *horizontal* cluster architecture as a single client-multiple server relationship:  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/scoverview.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two ways to process data in a distributed architecture:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. move data to code (& processing power)\n",
      "    - SETI\n",
      "2. move code to data\n",
      "    - map-reduce -> less overhead (network traffic, disk I/O)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> \u201cComputing nodes are the same as storage nodes.\u201d"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Divide and conquer is a fundamental algorithmic technique for solving a given task, whose steps include:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. split task into subtasks\n",
      "1. solve these subtasks independently\n",
      "1. recombine the subtask results into a final result"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One famous example of divide and conquer is *merge sort*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/Merge_sort_algorithm_diagram.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The defining characteristic of a problem that is suitable for the divide and conquer approach is that it can be broken down into independent subtasks."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tasks that can be parallelized in this way include:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* count, sum, average\n",
      "* grep, sort, inverted index\n",
      "* graph traversals, some ML algorithms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What if you can't afford a cluster?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Amazon Web Services"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/AWS-Overview-of-Services.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- IAM: Identity and Access Management\n",
      "- S3: Simple Storage Service\n",
      "- EC2: Elastic Cloud Compute\n",
      "- EMR: Elastic MapReduce"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Regions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* us-east-1 (North Virginia)\n",
      "* us-west-1 (Northern California)\n",
      "* ap-southeast-1 (Singapore)\n",
      "* eu-west-1 (Ireland)\n",
      "* also other regions in Asia, Europe, North and South America"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Availability Zones (AZ)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Each region has several availability zones.\n",
      "\n",
      "AZs should rarely matter, but important to know what it is (and not confuse AZs with regions). AZ will rarely matter. Region often will."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Types of authentication"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Warning: the word *key* is used a lot to mean different things.)\n",
      " \n",
      "1. Key & Secret\n",
      "    - Key = 20 character long token (all upper-case)\n",
      "    - Secret = 40 character long string\n",
      "    - Used for all AWS\n",
      "    - Created and managed in IAM\n",
      "    - Not region specific\n",
      "2. Keypair\n",
      "    - RSA encrypted\n",
      "    - Used specifically for SSH into EC2 (including EMR)\n",
      "    - Region specific\n",
      "    - Created and managed in EC2\n",
      "3. Other forms of authentication\n",
      "    - Alternative to Key & Secret\n",
      "    - Managed in IAM\n",
      "    - Useful for granting others partial access to your AWS resources  \n",
      "        (i.e. giving a friend write access to a specific S3 bucket)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "S3 (Simple Storage Service)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Sort of a file system\n",
      "    - Buckets ~ Folders\n",
      "    - Keys ~ Files (Note: a different meaning for the word *key* in this context)\n",
      "    - Subdirectories aren't really folders, but rather just prefixes to the key name\n",
      "- Region Specific\n",
      "    - but it usually doesn't matter (except for latency)\n",
      "- Relatively Cheap\n",
      "- Complex Permission Structure (about as complex as a real file system)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "EC2 (Elastic Cloud Compute)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Instance Types"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Optimization\n",
      "    - Compute Optimized\n",
      "    - Memory Optimized\n",
      "    - Storage Optimized\n",
      "    - General Purpose\n",
      "    \n",
      "    \n",
      "- Pricing Examples:\n",
      "    - t1.micro\n",
      "        - 1 CPU\n",
      "        - 615 MB RAM\n",
      "        - \\$0.02 per Hour\n",
      "    - i2.8xlarge\n",
      "        - 32 CPUs\n",
      "        - 244 GB RAM\n",
      "        - \\$6.82 per Hour\n",
      "    - m3.xlarge \n",
      "        - 4 CPUs\n",
      "        - 15 GB RAM\n",
      "        - \\$0.28 per Hour\n",
      "        \n",
      "- Not all instance types available in all regions (e.g. High Storage Instances not available in us-west-1)\n",
      "- Prices may vary (e.g. instances in us-west-1 are often more expensive)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "AMI (Amazon Machine Images)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* A machine image is a copyable snapshot of an instance's contents and configuration\n",
      "* If you've ever run a virtual machine (e.g. VirtualBox, VMware, Parallels), you may be familiar\n",
      "* EC2 instances start as AMIs that are then *instantiated*\n",
      "* StarCluster uses AMIs extensively to preconfigure EC2 instances to create a cluster\n",
      "* A well-configured AMI can save a lot of time\n",
      "* AMIs are region specific. Copies to other regions receive new ID\n",
      "\n",
      "A well-configured AMI can save a lot of time because you don't have to install software on every instance after it's booted up (the software is effectively pre-installed).\n",
      "They can be copied, but only by the creator and it will get a new ID when that happens."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "EMR (Elastic MapReduce)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Hadoop in the Cloud "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Uses EC2 instances\n",
      "* Web interface for provisioning cluster\n",
      "* Installs Hadoop, etc. automatically\n",
      "* Installs Hive, Pig, HBase automatically if desired\n",
      "* Provides a web interface to running Hadoop Streaming jobs.\n",
      "* Charges a small premium on top of EC2 prices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One thing EMR does that StarCluster doesn't is it automatically configures Hadoop to be able to read your S3 buckets as if they were part of HDFS. This has to be done manually in StarCluster (or any other software that runs Hadoop on EC2 (e.g. Whirr, etc.))"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "StarCluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/scoverview.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "StarCluster comes out of the STAR program at MIT. STAR stands for \"Software Tools for Academics and Researchers\". It is used to quickly provision a cluster of EC2 instances. Like EMR, it automatically configures them to be used as a cluster (rather than as independent machines) with one controller and many workers. Unlike EMR, it does not have a GUI. Also, Hadoop is just one of many plugins for StarCluster. The most important plugin, however, is IPython Cluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "Wikipedia XML Data\n",
      "------------------------------------------\n",
      "A complete copy of all Wikimedia wikis, in the form of wikitext source and metadata embedded in XML.\n",
      "<table>\n",
      "          <tr>\n",
      "            <td class=\"metaLeft\">\n",
      "              Size:\n",
      "            </td>\n",
      "            <td class=\"metaRight\">\n",
      "              \n",
      "500GB</td>\n",
      "          </tr>\n",
      "          <tr>\n",
      "            <td class=\"metaLeft\">\n",
      "              Source:\n",
      "            </td>\n",
      "            <td class=\"metaRight\">\n",
      "              \n",
      "Wikimedia Foundation (http://download.wikipedia.org/backup-index.html)</td>\n",
      "          </tr>\n",
      "          <tr>\n",
      "            <td class=\"metaLeft\">\n",
      "              Created On: \n",
      "            </td>\n",
      "            <td class=\"metaRight\">\n",
      "              \n",
      "<span class=\"timestamp\">May 15, 2009 12:09 AM GMT</span>\n",
      " \n",
      "            </td>\n",
      "          </tr>\n",
      "          <tr>\n",
      "            <td class=\"metaLeft\">\n",
      "              Last Updated:\n",
      "            </td>\n",
      "            <td class=\"metaRight\">\n",
      "              \n",
      "<span class=\"timestamp\">September 29, 2009  1:09 AM GMT</span>\n",
      " \n",
      "            </td>\n",
      "          </tr>\n",
      "        </table>  \n",
      "        \n",
      "http://aws.amazon.com/datasets/Encyclopedic/2506"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Part I : Preliminary investigation of our dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!pip install boto"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Instructions: http://star.mit.edu/cluster/docs/latest/installation.html\n",
      "!pip install StarCluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "from boto.s3.connection import S3Connection\n",
      "from IPython.parallel import Client"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('/home/io/aws-credentials.txt') as f:\n",
      "    credentials = f.read().splitlines() \n",
      "    key, secret = credentials[0], credentials[1]\n",
      "    \n",
      "print key\n",
      "print secret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s3conn = S3Connection(key, secret)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wikipediaxml_keys = datasets.get_all_keys(prefix='wikipediaxml')\n",
      "len(wikipediaxml_keys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get the smallest data subset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[k.size for k in wikipediaxml_keys].index(\n",
      "    min(k.size for k in wikipediaxml_keys))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# N.B. This takes a few minutes...\n",
      "data = wikipediaxml_keys[60].get_contents_as_string()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's explore the XML document object model using Python's minidom module:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from xml.dom import minidom\n",
      "\n",
      "dom = minidom.parseString(data.split('\\r')[0])\n",
      "print dom.firstChild.toprettyxml()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title = dom.firstChild.getElementsByTagName('title')[0]\n",
      "title.lastChild.wholeText"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "titles1 = [title.lastChild.wholeText \n",
      "     for entry in data.split('\\r')[:-1] \n",
      "         for title in minidom.parseString(entry).firstChild.getElementsByTagName('title')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Python XML module has another way of exploring XML as well:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from xml.etree import ElementTree\n",
      "ElementTree.fromstring(data.split('\\r')[0]).find('title').text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "titles2 = [ElementTree.fromstring(entry).find('title').text for entry in data.split('\\r')[:-1]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that both of these are extremely slow compared to simple string parsing:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import HTMLParser\n",
      "parser = HTMLParser.HTMLParser()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "titles3 = [parser.unescape(entry.split(u'</title>')[0]) for entry in data.split(u'<title>')[1:]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles1 == titles2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles2 == titles3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(titles3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles3[::10000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Part 2: Moving to StarCluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run starcluster to initialise the config file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# copy/paste this into your terminal\n",
      "starcluster help\n",
      "# Select option [2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Edit the config file to reflect your AWS credentials"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```bash\n",
      "#############################################\n",
      "## AWS Credentials and Connection Settings ##\n",
      "#############################################\n",
      "[aws info]\n",
      "# This is the AWS credentials section (required).\n",
      "# These settings apply to all clusters\n",
      "# replace these with your AWS keys\n",
      "AWS_ACCESS_KEY_ID = #your_aws_access_key_id\n",
      "AWS_SECRET_ACCESS_KEY = #your_secret_access_key\n",
      "# replace this with your account number\n",
      "AWS_USER_ID= #your userid\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add the ipcluster plugin if you haven't already.\n",
      "\n",
      "Near the bottom of `.starcluster/config:`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```bash\n",
      "######################\n",
      "## Built-in Plugins ##\n",
      "######################\n",
      "# The following plugins ship with StarCluster and should work out-of-the-box.\n",
      "# Uncomment as needed. Don't forget to update your PLUGINS list!\n",
      "# See http://star.mit.edu/cluster/docs/latest/plugins for plugin details.\n",
      "# .\n",
      "# .\n",
      "# .\n",
      "[plugin ipcluster]\n",
      "SETUP_CLASS = starcluster.plugins.ipcluster.IPCluster\n",
      "# Enable the IPython notebook server (optional)\n",
      "ENABLE_NOTEBOOK = True\n",
      "# Set a password for the notebook for increased security\n",
      "# This is optional but *highly* recommended\n",
      "NOTEBOOK_PASSWD = a-secret-password\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "tinyurl.com/EC2calc\n",
      "\n",
      "In theory, 5 r3.4xlarge spot instances at 13\u00a2/hour should work but in practice, AWS has not let me reserve any r3.4xlarge (maybe you'll have better luck).\n",
      "Reserving 17 r3.xlarge instances didn't work for me either (despite the fact that AWS claims the default limit on that is 20) so I am stuck using the more expensive m3.2xlarge instances:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```bash\n",
      "###########################################\n",
      "## Defining Additional Cluster Templates ##\n",
      "###########################################\n",
      "# You can also define multiple cluster templates. You can either supply all\n",
      "# configuration options as with smallcluster above, or create an\n",
      "# EXTENDS=<cluster_name> variable in the new cluster section to use all\n",
      "# settings from <cluster_name> as defaults. Below are example templates that\n",
      "# use the EXTENDS feature:\n",
      "\n",
      "[cluster mediumcluster]\n",
      "# Declares that this cluster uses smallcluster as defaults\n",
      "EXTENDS=smallcluster\n",
      "# This section is the same as smallcluster except for the following settings:\n",
      "NODE_IMAGE_ID = ami-6b211202\n",
      "NODE_INSTANCE_TYPE = m3.2xlarge\n",
      "CLUSTER_SIZE = 17\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a new authentication key for starcluster to use"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "starcluster createkey mykey -o ~/.ssh/mykey.rsa"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start your new cluster:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!starcluster start -c mediumcluster my_cluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!starcluster runplugin ipcluster my_cluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run the ipcluster plugin to connect your IPythong notebook with the cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "url_file = os.path.expanduser('~/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json')\n",
      "sshkey = '/home/io/.ssh/mykey.rsa'\n",
      "client = Client(url_file, sshkey)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from boto.s3.connection import S3Connection\n",
      "from collections import defaultdict\n",
      "\n",
      "s3conn = S3Connection(key, secret)\n",
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "client = Client('/home/io/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json', sshkey='/home/io/.ssh/mykey.rsa')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dview = client.direct_view()\n",
      "len(client.ids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "%%bash\n",
      "ec2metadata --local-ipv4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import pandas as pd\n",
      "from boto.s3.connection import S3Connection\n",
      "from collections import defaultdict\n",
      "\n",
      "s3conn = S3Connection('AKIAINWG7BMA6JGUOFNQ', '5scDgCSrCnRWM3+XKQRadlWcgNjJQ2t0ZSx6nVwU')\n",
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then scatter the keys to our S3 files across our cluster:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dview.scatter('wikipediaxml_keys', [key.name for key in wikipediaxml_keys], dist='r')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we were unable to investigate our data locally (perhaps the download failed) we can do so on engine zero by specifying the target thus:\n",
      "(only do this if the above attempt at running get_contents_as_string failed)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px --targets 0\n",
      "data = datasets.get_key(wikipediaxml_keys[0]).get_contents_as_string()\n",
      "data.split('\\r')[:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`-t0` is shorthand for `--targets 0`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px -t0\n",
      "titles = [entry.split('</title>')[0] for entry in data.split('<title>')[1:]]\n",
      "len(titles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px -t0\n",
      "titles[::10000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we run our analysis in parallel across 135 CPUs on 17 nodes. This will take a while, but consider the fact that this would be impossible your laptop:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px \n",
      "import pandas as pd\n",
      "from boto.s3.connection import S3Connection\n",
      "from collections import defaultdict\n",
      "\n",
      "credentials = pd.read_csv('credentials.csv')\n",
      "\n",
      "s3conn = S3Connection(credentials['Access Key Id'][0], credentials['Secret Access Key'][0])\n",
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')\n",
      "\n",
      "title_lens = defaultdict(int)\n",
      "errors = []\n",
      "\n",
      "for key_name in wikipediaxml_keys:\n",
      "    wikipediaxml_key = datasets.get_key(key_name)\n",
      "    try:\n",
      "        data = wikipediaxml_key.get_contents_as_string()\n",
      "        titles = [entry.split('</title>')[0] for entry in data.split('<title>')[1:]]\n",
      "        for title in titles:\n",
      "            title_tokens = title.split()\n",
      "            title_length = len(title_tokens)\n",
      "            title_lens[title_length] += 1\n",
      "    except:\n",
      "        errors.append(key_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "Intermission\n",
      "------------------------------------------\n",
      "------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_lens = dview.gather('title_lens').get()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series = pd.DataFrame(title_lens).sum(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\n",
      "\u00a1\u00a1\u00a1DON'T FORGET TO TERMINATE YOUR CLUSTER BEFORE YOU LEAVE CLASS!!!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "starcluster terminate my_cluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MapReduce Programming Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall: There are two ways to process data in a distributed architecture:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. move data to code (& processing power)\n",
      "    - SETI\n",
      "2. move code to data\n",
      "    - map-reduce -> less overhead (network traffic, disk I/O)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> \u201cComputing nodes are the same as storage nodes.\u201d"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we\u2019ve discussed, the map-reduce approach involves splitting a problem into subtasks and processing these subtasks in parallel."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This takes place in two(three!) phases:\n",
      "\n",
      "* the mapper phase\n",
      "* shuffle/sort <- the secret sauce\n",
      "* the reducer phase"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/mrfigure.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/mapreduce_mapshuffle.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Map-reduce uses a functional programming paradigm. The data processing primitives are mappers and reducers, as we\u2019ve seen.\n",
      "\n",
      "mappers \u2013 filter & transform data\n",
      "reducers \u2013 aggregate results\n",
      "\n",
      "Thanks to [Michael Cvet](http://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/) for this example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = [1, 2, 3]\n",
      "b = [4, 5, 6, 7]\n",
      "c = [8, 9, 1, 2, 3]\n",
      "L = map(lambda x:len(x), [a, b, c])\n",
      "L"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = reduce(lambda x, y: x+y, L)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or if you want to be fancy and do it as a one-liners"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduce(lambda x, y: x+y, map(lambda x:len(x), [a, b, c]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The functional paradigm is good at describing how to solve a problem, but not very good at describing data manipulations (eg, relational joins)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It\u2019s possible to overlay the map-reduce framework with an additional declarative syntax. This makes operations like select & join easier to implement and less error prone."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Popular examples for Hadoop include **Pig** and **Hive**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Why Pig?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because I bet you can read the following script."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```pig\n",
      "-- An example pig script:\n",
      "users = load 'users.csv' as (username: chararray, age: int);\n",
      "users_1825 = filter users by age >= 18 and age <= 25;\n",
      "\n",
      "pages = load 'pages.csv' as (username: chararray, url: chararray);\n",
      "\n",
      "joined = join users_1825 by username, pages by username;\n",
      "grouped = group joined by url;\n",
      "summed = foreach grouped generate group as url, COUNT(joined) AS views;\n",
      "top_5 = limit sorted 5;\n",
      "\n",
      "store top_5 into 'top_5_sites.csv';\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, just for fun... the same calculation in vanilla Hadoop MapReduce."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/pig_02.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Implementation Details"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "MapReduce is\u2026\n",
      "\n",
      "    A. A programming model for processing big data\n",
      "\n",
      "    B. A data processing system created and used by Google\n",
      "\n",
      "    C. A feature of Hadoop\n",
      "\n",
      "    D. All of the above"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The implementation of MapReduce became popular when Google released a white paper in 2004 explaining how they did it.\n",
      "\n",
      "This architecture was then copied. The two most popular versions are:\n",
      "\n",
      "* **Apache\u2019s Hadoop**\n",
      "* **Disco** (bet you haven\u2019t heard of this one)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Disco more friendly but not as well supported. Hadoop has a huge community and so, while more difficult to use, is the standard.\n",
      "\n",
      "A map-reduce framework handles a lot of messy details for you:\n",
      "\n",
      "* parallelization & distribution (eg, input splitting)\n",
      "* partitioning (shuffle/sort/redirect)\n",
      "* fault-tolerance (fact: tasks/nodes will fail!)\n",
      "* I/O scheduling\n",
      "* status and monitoring\n",
      "\n",
      "This (along with the functional semantics) allows you to focus on solving the problem instead of accounting & housekeeping details."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Hadoop"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Hadoop** is a popular open-source Java-based implementation of the map-reduce framework (including file storage for input/output).\n",
      "\n",
      "You can download Hadoop and configure a set of machines to operate as a map-reduce cluster, or you can run it as a service via Amazon\u2019s Elastic Map-Reduce.\n",
      "\n",
      "Hadoop is written in Java, but the *Hadoop Streaming* utility allows client code to be supplied as executables (eg, written in any language).\n",
      "\n",
      "Data is replicated in the (distributed) file system across several nodes.\n",
      "\n",
      "This permits locality optimization (and fault tolerance) by allowing the mapper tasks to run on the same nodes where the data resides.\n",
      "\n",
      "So we move code to data (instead of data to code), thus avoiding a lot of network traffic and disk I/O."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/GP-HD_HDFS-4-2.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The *Google File System (GFS)* was developed alongside map-reduce to serve as the native file system for this type of processing.\n",
      "\n",
      "The Hadoop platform is bundled with an open-source implementation of this file system called HDFS.\n",
      "\n",
      "If you use Amazon EMR, you can use their file system (Amazon S3) as well. (What's wrong with this?)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**[3 Caveats](http://www.zdnet.com/bursting-the-big-data-bubble-7000002352/):**\n",
      "* Hadoop is not an RDBMS killer \n",
      "* Hives and Hive-nots \n",
      "* Real-time Hadoop? Not really."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MapReduce Ecosystem & Beyond"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part of the success of tools in the Hadoop ecosystem ... \n",
      " \n",
      " > \"lies in the now-evident difficulty and sharp learning curve involved in MapReduce, as it was originally defined, when applied to practical problems.\"\" [Pere Ferrera Bertran](http://www.datasalt.com/2012/02/tuple-mapreduce-beyond-the-classic-mapreduce/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Hadoop ecosystem at present is rich enough already that we have a variety of front end options available, from visual spreadsheet metaphors (**Big Sheets**) to SQL-style queries (**Hive**) with a web UI (**Beeswax**). No Java necessary."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider Apache **Giraph** and **Hama** when there is a lot of communication between nodes. Giraph is based on Google **Pregel**. Hama is based on **BSP**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Word Count Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "MapReduce processes data in terms of key-value pairs:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```bash\n",
      "input           <k1, v1>\n",
      "\n",
      "mapper          <k1, v1> -> <k2, v2>\n",
      "\n",
      "(partitioner)   <k2, v2> -> <k2, [all k2 values]>\n",
      "\n",
      "reducer         <k2, [all k2 values]> -> <k3, v3>\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/MapReduceWordCountOverview1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the following input, we can implement the \u201cHello World\u201d of map-reduce: a word count."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = {1: 'where',\n",
      "     2: 'where in',\n",
      "     3: 'where in the',\n",
      "     4: 'where in the world',\n",
      "     5: 'where in the world is',\n",
      "     6: 'where in the world is carmen',\n",
      "     7: 'where in the world is carmen sandiego'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, as for comparison, this is one way you could do it on a single machine:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      " \n",
      "import pandas as pd\n",
      " \n",
      "wordcount = defaultdict(int)\n",
      " \n",
      "for _, s in i.items():\n",
      "    for w in s.split():\n",
      "        wordcount[w] += 1\n",
      "\n",
      "reducer_results = pd.Series(wordcount)\n",
      "reducer_results.sort()\n",
      "reducer_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first processing primitive is the mapper, which filters & transforms the input data, and yields transformed key-value pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mapper(k1, v1):\n",
      "    '''k1: line number\n",
      "       v1: line contents (i.e. space-delimited string)'''\n",
      "    words = v1.split()  # split string into words\n",
      "    for word in words:\n",
      "        yield (word, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The mapper emits key-value pairs for each word encountered in the input data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import chain\n",
      "map_results = list(chain.from_iterable(map(mapper, i.keys(), i.values())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The partitioner is internal to the map-reduce framework, so we don\u2019t have to write this ourselves. It shuffles & sorts the mapper output, and redirects all intermediate results for a given key to a single reducer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we were using a map-reduce framework like Hadoop or Disco, the partitioner would be internal. But since we are building this from scratch, we will create our own partitioner using a defaultdict. The partitioner shuffles & sorts the mapper output, and redirects all intermediate results for a given key to a single reducer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "partition_results = defaultdict(list)\n",
      "for k, v in map_results:\n",
      "    partition_results[k].append(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.Series(partition_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, the reducer receives all values for a given key and aggregates (in this case, sums) the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reducer(k2, k2_vals):\n",
      "    '''k2: word\n",
      "       k2_vals: word counts'''\n",
      "    yield k2, sum(k2_vals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reducer output is aggregated & sorted by key."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer_results = dict(chain.from_iterable(\n",
      "    map(reducer, partition_results.keys(), partition_results.values())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Beyond Word Count"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Activity Stream Sessionization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal: collate user activity, splitting into different  sessions if user inactive for more than 5 minutes\n",
      "  \n",
      "  > Input format: timestamp, user_id"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mapper(self, key, line):\n",
      "    timestamp, user_id = line.split()\n",
      "    yield user_id, timestamp\n",
      "\n",
      "def reducer(self, uid, timestamps):\n",
      "    yield uid, sorted(timestamps)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Sample Output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"998\" [\"1384389407\", \"1384389417\", \"1384389422\", \n",
      "\"1384389425\", \"1384390407\", \"1384390417\", \n",
      "\"1384391416\", \"1384392410\", \"1384392416\", \n",
      "\"1384395420\", \"1384396405\"]\n",
      "\n",
      "\"999\" [\"1384388414\", \"1384388425\", \"1384389419\", \n",
      "\"1384389420\", \"1384390420\", \"1384391415\", \n",
      "\"1384391418\", \"1384393413\", \"1384393425\", \n",
      "\"1384394426\", \"1384395416\", \"1384396415\", \n",
      "\"1384396422\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MAX_SESSION_INACTIVITY = 60 * 5\n",
      "# ...\n",
      "def reducer(self, uid, timestamps):\n",
      "    timestamps = sorted(timestamps)\n",
      "    start_index = 0\n",
      "    for index, timestamp in enumerate(timestamps):\n",
      "        if index > 0:\n",
      "            if timestamp - timestamps[index-1] > MAX_SESSION_INACTIVITY:\n",
      "                yield uid, timestamps[start_index:index]\n",
      "                start_index = index\n",
      "    yield uid, timestamps[start_index:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Sample Output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"999\"[1384388414, 1384388425]\n",
      "\"999\"[1384389419, 1384389420]\n",
      "\"999\"[1384390420]\n",
      "\"999\"[1384391415, 1384391418]\n",
      "\"999\"[1384393413, 1384393425]\n",
      "\"999\"[1384394426]\n",
      "\"999\"[1384395416]\n",
      "\"999\"[1384396415, 1384396422]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Product Recommendations "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal: For each product a client sells, generate a \u2018people who bought this also bought this\u2019 recommendation\n",
      "\n",
      "  > Input: product_id_1, product_id_2, .."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Coincident Purchase Frequency\n",
      "def mapper(self, key, line):\n",
      "    purchases = set(line.split(','))\n",
      "    for p1, p2 in permutations(purchases, 2):\n",
      "        yield (p1, p2), 1\n",
      "\n",
      "def reducer(self, pair, occurrences):\n",
      "    p1, p2 = pair\n",
      "    yield p1, (p2, sum(occurrences))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Sample Output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"8\" [\"5\", 11]\n",
      "\"8\" [\"6\", 19]\n",
      "\"8\" [\"7\", 14]\n",
      "\"8\" [\"9\", 11]\n",
      "\"9\" [\"1\", 20]\n",
      "\"9\" [\"10\", 22]\n",
      "\"9\" [\"11\", 21]\n",
      "\"9\" [\"12\", 13]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reducer(self, purchase_pair, occurrences):\n",
      "    p1, p2 = purchase_pair\n",
      "    yield p1, (sum(occurrences), p2)\n",
      "\n",
      "def reducer_find_best_recos(self, p1, p2_occurrences):\n",
      "    top_products = sorted(p2_occurrences, reverse=True)[:5]\n",
      "    top_products = [p2 for occurrences, p2 in top_products]\n",
      "    yield p1, top_products\n",
      "\n",
      "def steps(self):\n",
      "    return [self.mr(mapper=self.mapper, reducer=self.reducer), self.mr(reducer=self.reducer_find_best_recos)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Sample Output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"7\" [\"15\", \"18\", \"17\", \"16\", \"3\"]\n",
      "\"8\" [\"14\", \"15\", \"20\", \"6\", \"3\"]\n",
      "\"9\" [\"15\", \"17\", \"19\", \"6\", \"3\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. User Behavior Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal: compute statistics about user behavior (conversion rate & time on site) by account and experiment in an efficient manner\n",
      "\n",
      "> Input: account_id, campaigns_viewed, user_id, purchased?, session_start_time, session_end_time\n",
      "\n",
      "For full details on this and other examples see [Jeff Patti](http://www.slideshare.net/JeffPatti/map-reducebeyondwordcount)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hadoop Streaming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "(Try this at) Homework"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git clone https://github.com/ogrisel/parallel_ml_tutorial.git '../../parallel_ml_tutorial'    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Go through Olivier Grisel's Parallel Machine Learning with scikit-learn and IPython tutorials. Much of it will be a review. Pay attention to how 03 - Distributed Model Selection and Assessment shows you how to do Grid Search over a StarCluster. But the coup de gr\u00e2ce is 05 - Large Scale Text Classification for Sentiment Analysis which shows how to build a classification model using a large dataset in parallel across a cluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/resources.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Without Context, Data is Meaningless](http://dannybrown.me/2013/08/15/without-context-data-is-meaningless/)\n",
      "* [Using s3 instead of HDFS in the Cloud](http://www.technology-mania.com/2012/05/s3-instead-of-hdfs-with-hadoop_05.html)\n",
      "* [Tuple MapReduce: beyond the classic MapReduce](http://www.datasalt.com/2012/02/tuple-mapreduce-beyond-the-classic-mapreduce/)\n",
      "* [MapReduce & Hadoop API revised](http://www.datasalt.com/2012/02/mapreduce-hadoop-problems/)\n",
      "* [How Twitter Uses NoSQL\n",
      "](http://readwrite.com/2011/01/02/how-twitter-uses-nosql#awesm=~oINrJQljtJRO57)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Python Documentation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Parallel Magic Commands](http://ipython.org/ipython-doc/dev/parallel/magics.html)\n",
      "* [StarCluster Documentation](https://media.readthedocs.org/pdf/starcluster/latest/starcluster.pdf)\n",
      "* [STARCluster QuickStart](http://star.mit.edu/cluster/docs/latest/quickstart.html)\n",
      "* [IPython Cluster Plugin](http://star.mit.edu/cluster/docs/latest/plugins/ipython.html)\n",
      "* [A Guide to Python Frameworks for Hadoop](http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/)\n",
      "* [mrjob: lets you write MapReduce jobs in Python ](https://pythonhosted.org/mrjob/)\n",
      "* [Dumbo Tutorial](https://github.com/klbostee/dumbo/wiki/Short-tutorial)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}