{
 "metadata": {
  "name": "",
  "signature": "sha256:d918b46f2e9050845a8f8c0306314ddf8298a5d9592c6144f200e69c90ad2499"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%javascript\n",
      "function is_local(){\n",
      "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
      "}\n",
      "var url = is_local() ? \"http://localhost:8000/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
      "$.getScript(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "function is_local(){\n",
        "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
        "}\n",
        "var url = is_local() ? \"http://localhost:8000/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
        "$.getScript(url)"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Javascript at 0x2e8f850>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Querying Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](http://imgs.xkcd.com/comics/exploits_of_a_mom.png)\n",
      "> \n",
      "\n",
      "<footer>~ XKCD</footer>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/agenda.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Agenda"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. [Intro To Databases](#Intro-To-Databases)\n",
      "1. [Relational Databases](#Relational-Databases)\n",
      "\n",
      "**Labs:**\n",
      "1. [SQL Queries](#Lab:-SQL-Queries)\n",
      "1. [Pandas Workshop](#Pandas-Workshop)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Intro To Databases"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Databases are a **structured** data source optimized for\n",
      "efficient **retrieval and storage**.\n",
      "\n",
      "* **structured**: we have to pre-define organization strategy\n",
      "* **retrieval**: the ability to read data out\n",
      "* **storage**: the ability to write data and save it"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Relational Databases"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Relational databases are traditionally organized in the\n",
      "following manner:\n",
      "    \n",
      "* A database has tables which represent individual entities\n",
      "or objects\n",
      "* Tables have predefined schema \u2013 rules that tell it what\n",
      "the data will look like"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/relational-databases_1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could have had a table structure as follows:\n",
      "Why is this different?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/relational-databases_3.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We would repeat the user information in each row. This is called denormalization."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Normalised vs Denormalised"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Normalized Data**: Many tables to reduce redundant or\n",
      "repeated data in a table\n",
      "\n",
      "**Denormalized Data**: Wide data with fields often repeated\n",
      "but removes the need to join together multiple tables\n",
      "\n",
      "**This is a trade off of speed vs storage.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/relational-databases_4.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Q: How do we commonly evaluate databases?**\n",
      "    \n",
      "* read-speed vs write-speed\n",
      "* space considerations\n",
      "* (and many, many other criteria)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Q: Why are normalized tables (possibly) slower to read?**\n",
      "    \n",
      "A: We\u2019ll have to get data from multiple tables to answer\n",
      "some questions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Q: Why are denormalized tables (possibly) slower to write?**\n",
      "\n",
      "A: We\u2019ll have to write more information on each write"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "SQL"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SQL is a query language to load, retrieve, and update data in relational databases\n",
      "\n",
      "Most commonly known SQL-like Databases include:\n",
      "* Oracle\n",
      "* MySQL\n",
      "* PostgreSQL"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**SELECT: Allows you to retrieve information from a table**\n",
      "\n",
      "Syntax:\n",
      "\n",
      "```SQL\n",
      "SELECT col1, col2\n",
      "FROM table WHERE [some condition]\n",
      "```\n",
      "\n",
      "Example:\n",
      "\n",
      "```SQL\n",
      "SELECT poll_title, poll_date FROM polls WHERE\n",
      "romney_pct > obama_pct\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**GROUP BY: Allows you to aggregate information.**\n",
      "\n",
      "Syntax:\n",
      "\n",
      "    SELECT col1, AVG(col2)\n",
      "    FROM table GROUP BY col1\n",
      "\n",
      "Example:\n",
      "\n",
      "    SELECT poll_date, AVG(obama_pct)\n",
      "    FROM polls GROUP BY poll_date"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**GROUP BY: Allows you to aggregate information.**\n",
      "\n",
      "Syntax:\n",
      "\n",
      "    SELECT col1, AVG(col2)\n",
      "    FROM table GROUP BY col1\n",
      "\n",
      "Example:\n",
      "\n",
      "    SELECT poll_date, AVG(obama_pct)\n",
      "    FROM polls GROUP BY poll_date"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**GROUP BY: Allows you to aggregate information.**\n",
      "\n",
      "Syntax:\n",
      "\n",
      "    SELECT col1, AVG(col2)\n",
      "    FROM table GROUP BY col1\n",
      "\n",
      "There are usually a few common built-in operations:\n",
      "\n",
      "    SUM, AVG, MIN, MAX, COUNT"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** JOIN: Allows you to combine multiple tables**\n",
      "\n",
      "Syntax:\n",
      "\n",
      "    SELECT t1.c1, t1.c2, t2.c2\n",
      "    FROM t1 JOIN t2 ON t1.c1 = t2.c2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**JOIN: Allows you to combine multiple tables**\n",
      "    \n",
      "Syntax:\n",
      "\n",
      "    SELECT t1.c1, t1.c2, t2.c2\n",
      "    FROM t1 JOIN t2 ON t1.c1 = t2.c2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**INSERT: Allows you to add data to tables**\n",
      "\n",
      "Syntax:\n",
      "\n",
      "    INSERT INTO table1 (col1, col2)\n",
      "    VALUES (...)\n",
      "    INSERT INTO classroom (first_name, last_name)\n",
      "    VALUES (\u2018John\u2019, \u2018Doe\u2019)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Lab: SQL Queries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll be playing with the live database available at [W3Schools](http://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Questions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's walk through a few examples:\n",
      "\n",
      "**1) Retrieve all Customers from Madrid**\n",
      "\n",
      "```SQL\n",
      "SELECT * FROM Customers WHERE City='Madrid'\n",
      "```\n",
      "\n",
      "**2) What is the most common city for customers?**\n",
      "\n",
      "```SQL\n",
      "SELECT City, COUNT(*) FROM Customers GROUP BY City\n",
      "```\n",
      "\n",
      "**3) What category has the most products?**\n",
      "\n",
      "```SQL\n",
      "SELECT CategoryName, COUNT(*) FROM Categories\n",
      "JOIN Products on (Categories.CategoryID = Products.CategoryID)\n",
      "GROUP BY CategoryName\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classwork"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. What customers are from the UK\n",
      "2. What is the name of the customer who has the most orders?\n",
      "3. What supplier has the highest average product price?\n",
      "4. What category has the most orders?\n",
      "5. What employee made the most sales (by number of sales)?\n",
      "6. What employee made the most sales (by value of sales)?\n",
      "7. What employees have BS degrees? (Hint: Look at LIKE operator)\n",
      "8. What supplier has the highest average product price *assuming they have at least 2 products* (Hint: Look at the HAVING operator)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Submit these SQL queries as a .sql file to GitHub, using SQL comments to have the question referring to each:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```SQL\n",
      "-- What customers are from the UK?\n",
      "SELECT * FROM Customers WHERE Country = 'UK'\n",
      "\n",
      "-- What is the name of the customer who has the most orders?\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Pandas Workshop"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that you've added some SQL to your toolbelt, it would be wonderful if you could also use that knowledge in python. Though there are quite a few advanced libraries to [deal with SQL in python](http://www.sqlalchemy.org/), for most applications where data volumes are managable, _pandas dataframes_ offer functionalities similar to SQL. So let's see how we can use Pandas to splice and dice our data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A `DataFrame` is a tabular data structure, encapsulating multiple series like columns in a spreadsheet. Data are stored internally as a 2-dimensional object, but the `DataFrame` allows us to represent and manipulate higher-dimensional data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Set some Pandas options\n",
      "pd.set_option('max_columns', 30)\n",
      "pd.set_option('max_rows', 20)\n",
      "\n",
      "# Store data in a consistent place\n",
      "\n",
      "DATA_DIR = '../data/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import the Aggregated NTY data we built up in Lesson 3.\n",
      "\n",
      "df = pd.read_csv(DATA_DIR + 'nyt_agg.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The tabular data has both a primary `index` for rows, and a secondary index for `columns`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can rename the columns by either writing directly to the `columns` property or using the built in `rename()` method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.columns = [u'ERROR', u'Gender', u'Impressions', u'Clicks', u'Signed_In']\n",
      "df.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.rename(columns={'ERROR':'Age'}, inplace=True)\n",
      "df.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we wish to access columns, we can do so either by dict-like indexing or by attribute:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['Age']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.Age"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While these may look the same on the surface, when accessed with the dotnation the columns in Pandas DataFrames returns a Series - another Pandas datatype. When using the bracket notation it will just return another DataFrame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(df.Age)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(df[['Age']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This difference means that than with `Series`, we can use dict-like indexing to retrieve a particular element (row). But if we want access to a row in a `DataFrame`, we retrieve an index using its `ix` attribute.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.Age[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['Age'].ix[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['Age'][3] # This retrieves the third row, instead of the row with the label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Its important to note that the Series returned when a DataFrame is indexed is merely a **view** on the DataFrame, and not a copy of the data itself. So you must be cautious when manipulating this data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ages = df.Age\n",
      "ages"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ages[:10] = 0\n",
      "ages"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.Age"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you wanted to avoid creating a view and would instead prefer to **copy** the data out, the `copy()` method is your friend."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ages = df.Age.copy()\n",
      "ages[:10] = range(10)\n",
      "ages"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.Age"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can create or modify columns by assignment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['Age'][0] = 49\n",
      "df['Age']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['site'] = 'NYT'\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But note, we cannot use the attribute indexing method to add a new column:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.month = 'may'\n",
      "df.month"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Other Python data structures (ones without an index) need to be the same length as the `DataFrame`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "month = ['Jan', 'Feb', 'Mar', 'Apr']\n",
      "data['month'] = month"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can extract the underlying data as a simple `ndarray` by accessing the `values` attribute:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that because of the mix of strings and integer values, the dtype of the array is object. The dtype will automatically be chosen to be as general as needed to accomodate all the columns."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.values.dtype"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Indexing and Selection\n",
      "\n",
      "Indexing works analogously to indexing in NumPy arrays, except we can use the labels in the `Index` object to extract values in addition to arrays of integers. Let's use some baseball data to exemplify."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!curl https://raw.githubusercontent.com/fonnesbeck/pytenn2014_tutorial/master/data/baseball.csv > ../data/baseball.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball = pd.read_csv(DATA_DIR + 'baseball.csv', index_col='id')\n",
      "baseball"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could have easily chosen our own index label, by combining the playerID with the year they were active in"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "player_id = baseball.player + baseball.year.astype(str)\n",
      "baseball_x = baseball.copy()\n",
      "baseball_x.index = player_id\n",
      "baseball_x.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We can then use the label to access a record\n",
      "baseball_x.ix['aloumo012007']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's focus on one feature, and sample a Series object\n",
      "hits = baseball_x.h\n",
      "hits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Numpy-style indexing\n",
      "hits[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Numpy-style slice indexing\n",
      "hits[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Indexing by label\n",
      "hits[['womacto012006','schilcu012006']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also slice with data labels, since they have an intrinsic order within the Index:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hits.ix['womacto012006':'gonzalu012006']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a `DataFrame` we can slice along either or both axes:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball_x[['h','ab']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The indexing field `ix` allows us to select subsets of rows and columns in an intuitive way:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball_x.ix['gonzalu012006', ['h','X2b', 'X3b', 'hr']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball_x.ix[['gonzalu012006','finlest012006'], 5:8]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball_x.ix[:'myersmi012006', 'hr']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/voronoi.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classwork"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Create an index for the baseball data set which incoporates the team name."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Operations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`DataFrame` and `Series` objects allow for several operations to take place either on a single object, or between two or more objects.\n",
      "\n",
      "For example, we can perform arithmetic on the elements of two objects, such as combining baseball statistics across years. Let's try and see which players appeared in both 2006 and 2007 seasons (in this limited dataset)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball.year==2006"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use the baseball.year==2006 as a selection mask, xs to select one column\n",
      "hr2006 = baseball[baseball.year==2006].xs('hr', axis=1)\n",
      "# or\n",
      "hr2006 = baseball[baseball.year==2006]['hr']\n",
      "\n",
      "# by creating our own index so we can compare it with data from 2007\n",
      "hr2006.index = baseball.player[baseball.year==2006]\n",
      "\n",
      "# or all in one statement\n",
      "\n",
      "hr2006 = pd.Series(baseball.hr[baseball.year==2006].values, \n",
      "                   index=baseball.player[baseball.year==2006])\n",
      "\n",
      "hr2006"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hr2007 = baseball[baseball.year==2007].xs('hr', axis=1)\n",
      "hr2007.index = baseball.player[baseball.year==2007]\n",
      "hr2007"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# hr2006 = pd.Series(baseball.hr[baseball.year==2006].values, index=baseball.player[baseball.year==2006])\n",
      "type(hr2006)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hr_total = hr2006 + hr2007\n",
      "hr_total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pandas' data alignment places `NaN` values for labels that do not overlap in the two Series. In fact, there are only 6 players that occur in both years."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hr_total[hr_total.notnull()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "While we do want the operation to honor the data labels in this way, we probably do not want the missing values to be filled with `NaN`. We can use the `add` method to calculate player home run totals by using the `fill_value` argument to insert a zero for home runs where labels do not overlap:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hr2007.add(hr2006, fill_value=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "broadcasting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Operations can also be **broadcast** between rows or columns.\n",
      "\n",
      "For example, if we subtract the maximum number of home runs hit from the `hr` column, we get how many fewer than the maximum were hit by each player:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball.hr - baseball.hr.max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or, looking at things row-wise, we can see how a particular player compares with the rest of the group with respect to important statistics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print baseball.ix[89521][\"player\"]\n",
      "stats = baseball[['h','X2b', 'X3b', 'hr']]\n",
      "diff = stats - stats.xs(89521)\n",
      "diff[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also apply functions to each column or row of a `DataFrame`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stats.apply(np.median)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stat_range = lambda x: x.max() - x.min()\n",
      "stats.apply(stat_range)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "slg = lambda x: (x['h']-x['X2b']-x['X3b']-x['hr'] + 2*x['X2b'] + 3*x['X3b'] + 4*x['hr'])/(x['ab']+1e-6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stats = baseball[['h','X2b', 'X3b', 'hr','ab']]\n",
      "# baseball['SLG'] = stats.apply(slg)\n",
      "baseball['SLG'] = stats.apply(slg, axis=1)\n",
      "\n",
      "baseball['SLG_Diff'] = baseball['SLG'] - baseball['SLG'].max()\n",
      "baseball['SLG_Diff'].hist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sorting and Ranking"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pandas objects include methods for re-ordering data.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball_x.sort_index().head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball_x.sort_index(ascending=False).head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball_x.sort_index(axis=1).head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also use `order` to sort a `Series` by value, rather than by label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball.hr.order(ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a `DataFrame`, we can sort according to the values of one or more columns using the `by` argument of `sort_index`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball[['player','sb','cs']].sort_index(ascending=[False,True], by=['sb', 'cs']).head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Ranking"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Ranking** does not re-arrange data, but instead returns an index that ranks each value relative to others in the Series."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball.hr.rank()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ties are assigned the mean value of the tied ranks, which may result in decimal values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.Series([100,100]).rank()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alternatively, you can break ties via one of several methods, such as by the order in which they occur in the dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball.hr.rank(method='first')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calling the `DataFrame`'s `rank` method results in the ranks of all columns:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball.rank(ascending=False).head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseball[['r','h','hr']].rank(ascending=False).head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Missing data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The occurence of missing data is so prevalent that it pays to use tools like Pandas, which seamlessly integrates missing data handling so that it can be dealt with easily, and in the manner required by the analysis at hand.\n",
      "\n",
      "Missing data are represented in `Series` and `DataFrame` objects by the `NaN` floating point value. However, `None` is also treated as missing, since it is commonly used as such in other contexts (*e.g.* NumPy)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "foo = pd.Series([np.nan, -3, None, 'foobar'])\n",
      "foo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "foo.isnull()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Missing values may be dropped or indexed out:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.DataFrame({'value':[632, 1638, 569, 115, None, 1130, 754, 555],\n",
      "                     'patient':[1, 1, np.nan, 1, 2, 2, 2, 2],\n",
      "                     'treatment' : [0,0,0,0,1,1,np.nan,np.nan],\n",
      "                     'phylum':['Firmicutes', 'Proteobacteria', 'Actinobacteria', \n",
      "    'Bacteroidetes', 'Firmicutes', 'Proteobacteria', 'Actinobacteria', 'Bacteroidetes']})\n",
      "data['year'] = 2013\n",
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.dropna()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[data['patient'].notnull()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default, `dropna` drops entire rows in which one or more values are missing. This can be overridden by passing the `how='all'` argument, which only drops a row when every field is a missing value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.dropna(how='all')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This can be customized further by specifying how many values need to be present before a row is dropped via the `thresh` argument."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.ix[7, 'year'] = np.nan\n",
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.dropna(thresh=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is typically used in time series applications, where there are repeated measurements that are incomplete for some subjects."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we want to drop missing values column-wise instead of row-wise, we use `axis=1`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.dropna(axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Rather than omitting missing data from an analysis, in some cases it may be suitable to fill the missing value in, either with a default value (such as zero) or a value that is either imputed or carried forward/backward from similar data points. We can do this programmatically in Pandas with the `fillna` argument."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.fillna(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.fillna({'patient': 0, 'year': 2013, 'treatment':2})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that `fillna` by default returns a new object with the desired filling behavior, rather than changing the `Series` or  `DataFrame` in place (**in general, we like to do this, by the way!**)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Missing values can also be interpolated, using any one of a variety of methods:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data['patient'] = data['patient'].fillna(method='bfill')\n",
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data['value'] = data['value'].fillna(data['value'].mean())\n",
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Merging and joining DataFrame objects"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, we will manipulate data collected from ocean-going vessels on the eastern seaboard. Vessel operations are monitored using the Automatic Identification System (AIS), a safety at sea navigation technology which vessels are required to maintain and that uses transponders to transmit very high frequency (VHF) radio signals containing static information including ship name, call sign, and country of origin, as well as dynamic information unique to a particular voyage such as vessel location, heading, and speed. \n",
      "\n",
      "The International Maritime Organization\u2019s (IMO) International Convention for the Safety of Life at Sea requires functioning AIS capabilities on all vessels 300 gross tons or greater and the US Coast Guard requires AIS on nearly all vessels sailing in U.S. waters. The Coast Guard has established a national network of AIS receivers that provides coverage of nearly all U.S. waters. AIS signals are transmitted several times each minute and the network is capable of handling thousands of reports per minute and updates as often as every two seconds. Therefore, a typical voyage in our study might include the transmission of hundreds or thousands of AIS encoded signals. This provides a rich source of spatial data that includes both spatial and temporal information.\n",
      "\n",
      "For our purposes, we will use summarized data that describes the transit of a given vessel through a particular administrative area. The data includes the start and end time of the transit segment, as well as information about the speed of the vessel, how far it travelled, etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!curl https://raw.githubusercontent.com/fonnesbeck/pytenn2014_tutorial/master/data/AIS/transit_segments.csv > ../data/transit_segments.csv\n",
      "segments = pd.read_csv(DATA_DIR + 'transit_segments.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "segments.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In addition to the behavior of each vessel, we may want a little more information regarding the vessels themselves. In the `data/AIS` folder there is a second table that contains information about each of the ships that traveled the segments in the `segments` table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!curl https://raw.githubusercontent.com/fonnesbeck/pytenn2014_tutorial/master/data/AIS/vessel_information.csv > ../data/vessel_information.csv\n",
      "vessels = pd.read_csv(DATA_DIR + \"vessel_information.csv\", index_col='mmsi')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vessels.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vessels.type.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The challenge, however, is that several ships have travelled multiple segments, so there is not a one-to-one relationship between the rows of the two tables. The table of vessel information has a *one-to-many* relationship with the segments.\n",
      "\n",
      "In Pandas, we can combine tables according to the value of one or more *keys* that are used to identify rows, much like an index. Using a trivial example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df1 = pd.DataFrame(dict(id=range(4), age=np.random.randint(18, 31, size=4)))\n",
      "df2 = pd.DataFrame(dict(id=range(3)+range(3), score=np.random.random(size=6)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df1, df2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.merge(df1, df2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that without any information about which column to use as a key, Pandas did the right thing and used the `id` column in both tables. Unless specified otherwise, `merge` will used any common column names as keys for merging the tables. \n",
      "\n",
      "Notice also that `id=3` from `df1` was omitted from the merged table. This is because, by default, `merge` performs an **inner join** on the tables, meaning that the merged table represents an intersection of the two tables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.merge(df1, df2, how='outer')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The **outer join** above yields the union of the two tables, so all rows are represented, with missing values inserted as appropriate. One can also perform **right** and **left** joins to include all rows of the right or left table (*i.e.* first or second argument to `merge`), but not necessarily the other.\n",
      "\n",
      "Looking at the two datasets that we wish to merge:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "segments.head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vessels.head(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we see that there is a `mmsi` value (a vessel identifier) in each table, but it is used as an index for the `vessels` table. In this case, we have to specify to join on the index for this table, and on the `mmsi` column for the other."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "segments_m = pd.merge(vessels, segments, left_index=True, right_on='mmsi')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "segments_m.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, the default inner join is suitable; we are not interested in observations from either table that do not have corresponding entries in the other. \n",
      "\n",
      "Notice that `mmsi` field that was an index on the `vessels` table is no longer an index on the merged table.\n",
      "\n",
      "Here, we used the `merge` function to perform the merge; we could also have used the `merge` method for either of the tables:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vessels.merge(segments, left_index=True, right_on='mmsi').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Occasionally, there will be fields with the same in both tables that we do not wish to use to join the tables; they may contain different information, despite having the same name. In this case, Pandas will by default append suffixes `_x` and `_y` to the columns to uniquely identify them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "segments['type'] = 'foo'\n",
      "pd.merge(vessels, segments, left_index=True, right_on='mmsi').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This behavior can be overridden by specifying a `suffixes` argument, containing a list of the suffixes to be used for the columns of the left and right columns, respectively."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Concatenation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A common data manipulation is appending rows or columns to a dataset that already conform to the dimensions of the exsiting rows or colums, respectively. In NumPy, this is done either with concatenate or the convenience functions c_ and r_:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.concatenate([np.random.random(5), np.random.random(5)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.r_[np.random.random(5), np.random.random(5)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.c_[np.random.random(5), np.random.random(5)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This operation is also called **binding** or **stacking**.\n",
      "\n",
      "With Pandas' indexed data structures, there are additional considerations as the overlap in index values between two data structures affects how they are concatenate.\n",
      "\n",
      "Lets import two microbiome datasets, each consisting of counts of microorganiams from a particular patient. We will use the first column of each dataset as the index."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!curl https://raw.githubusercontent.com/fonnesbeck/pytenn2014_tutorial/master/data/microbiome/MID1.xls > ../data/MID1.xls\n",
      "mb1 = pd.read_excel(DATA_DIR + 'MID1.xls', 'Sheet 1', index_col=0, header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!curl https://raw.githubusercontent.com/fonnesbeck/pytenn2014_tutorial/master/data/microbiome/MID2.xls > ../data/MID2.xls\n",
      "mb2 = pd.read_excel(DATA_DIR + 'MID2.xls', 'Sheet 1', index_col=0, header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb1.shape, mb2.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb1.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb1.columns = mb2.columns = ['Count']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb1.index.name = mb2.index.name = 'Taxon'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb1.head()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The index of these data is the unique biological classification of each organism, beginning with domain, phylum, class, and for some organisms, going all the way down to the genus level."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](http://4.bp.blogspot.com/-mxgTyt7mjqk/T727bsIiomI/AAAAAAAAAKU/Z89lPUkhPfU/s1600/200px-Biological_classification_L_Pengo_vflip.svg.png)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb1.index[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb1.index.is_unique"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.concat([mb1, mb2], axis=0).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, the index is no longer unique, due to overlap between the two DataFrames."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.concat([mb1, mb2], axis=0).index.is_unique"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.concat([mb1, mb2], axis=1).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.concat([mb1, mb2], axis=1).head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.concat([mb1, mb2], axis=1).values[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we are only interested in taxa that are included in both DataFrames, we can specify a join=inner argument."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.concat([mb1, mb2], axis=1, join='inner').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mb1.combine_first(mb2).head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alternatively, you can pass keys to the concatenation by supplying the DataFrames (or Series) as a dict."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.concat(dict(patient1=mb1, patient2=mb2), axis=1).head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you want concat to work like numpy.concatanate, you may provide the ignore_index=True argument."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Reshaping DataFrame objects"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the context of a single DataFrame, we are often interested in re-arranging the layout of our data. \n",
      "\n",
      "This dataset in from Table 6.9 of [Statistical Methods for the Analysis of Repeated Measurements](http://www.amazon.com/Statistical-Methods-Analysis-Repeated-Measurements/dp/0387953701) by Charles S. Davis, pp. 161-163 (Springer, 2002). These data are from a multicenter, randomized controlled trial of botulinum toxin type B (BotB) in patients with cervical dystonia from nine U.S. sites.\n",
      "\n",
      "* Randomized to placebo (N=36), 5000 units of BotB (N=36), 10,000 units of BotB (N=37)\n",
      "* Response variable: total score on Toronto Western Spasmodic Torticollis Rating Scale (TWSTRS), measuring severity, pain, and disability of cervical dystonia (high scores mean more impairment)\n",
      "* TWSTRS measured at baseline (week 0) and weeks 2, 4, 8, 12, 16 after treatment began"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!curl https://raw.githubusercontent.com/fonnesbeck/pytenn2014_tutorial/master/data/cdystonia.csv > ../data/cdystonia.csv\n",
      "cdystonia = pd.read_csv(\"../data/cdystonia.csv\", index_col=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This dataset includes repeated measurements of the same individuals (longitudinal data). Its possible to present such information in (at least) two ways: showing each repeated measurement in their own row, or in multiple columns representing mutliple measurements.\n",
      "\n",
      "The `stack` method rotates the data frame so that columns are represented in rows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stacked = cdystonia.stack()\n",
      "stacked"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To complement this, `unstack` pivots from rows back to columns."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stacked.unstack().head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this dataset, it makes sense to create a hierarchical index based on the patient and observation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia2 = cdystonia.set_index(['patient','obs'])\n",
      "cdystonia2.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia2.index.is_unique"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we want to transform this data so that repeated measurements are in columns, we can `unstack` the `twstrs` measurements according to `obs`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twstrs_wide = cdystonia2['twstrs'].unstack('obs')\n",
      "twstrs_wide.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia_long = cdystonia[['patient','site','id','treat','age','sex']].drop_duplicates().merge(\n",
      "                    twstrs_wide, right_index=True, left_on='patient', how='inner').head()\n",
      "cdystonia_long"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A slightly cleaner way of doing this is to set the patient-level information as an index before unstacking:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia.set_index(['patient','site','id','treat','age','sex','week'])['twstrs'].unstack('week').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To convert our \"wide\" format back to long, we can use the `melt` function, appropriately parameterized:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.melt(cdystonia_long, id_vars=['patient','site','id','treat','age','sex'], \n",
      "        var_name='obs', value_name='twsters').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This illustrates the two formats for longitudinal data: **long** and **wide** formats. Its typically better to store data in long format because additional data can be included as additional rows in the database, while wide format requires that the entire database schema be altered by adding columns to every row as data are collected.\n",
      "\n",
      "The preferable format for analysis depends entirely on what is planned for the data, so it is imporant to be able to move easily between them."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data transformation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a slew of additional operations for DataFrames that we would collectively refer to as \"transformations\" that include tasks such as removing duplicate values, replacing values, and grouping values."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Dealing with duplicates"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can easily identify and remove duplicate values from `DataFrame` objects. For example, say we want to removed ships from our `vessels` dataset that have the same name:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vessels.duplicated(cols='names')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vessels.drop_duplicates(['names'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Value Replacement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Frequently, we get data columns that are encoded as strings that we wish to represent numerically for the purposes of including it in a quantitative analysis. For example, consider the treatment variable in the cervical dystonia dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia.treat.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A logical way to specify these numerically is to change them to integer values, perhaps using \"Placebo\" as a baseline value. If we create a dict with the original values as keys and the replacements as values, we can pass it to the `map` method to implement the changes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "treatment_map = {'Placebo': 0, '5000U': 1, '10000U': 2}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia['treatment'] = cdystonia.treat.map(treatment_map)\n",
      "cdystonia.treatment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alternately, if we simply want to replace particular values in a `Series` or `DataFrame`, we can use the `replace` method. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia2.treat.replace({'Placebo': 0, '5000U': 1, '10000U': 2})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data aggregation and GroupBy operations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For aggregation tasks, the `groupby` function is the most useful method in pandas. When performing a `groupby` operation, we may different goals:\n",
      "\n",
      "* Perform an **aggregation**, like computing the sum of mean of each group. Functionally this means applying a function to each group and putting the aggregated results into a DataFrame\n",
      "* **Slicing** the DataFrame into chunks (groups) and then doing something with the slices\n",
      "* Performing a **transformation**, like standardizing each group (computing the zscore)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So there are two tasks: first, grouping the data; second, doing something with the grouped data. As far as grouping\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia_grouped = cdystonia.groupby(cdystonia.patient)\n",
      "cdystonia_grouped"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This returns an object of type GroupBy - making it hard to visualise. However, the grouping is only an intermediate step; for example, we may want to **iterate** over each of the patient groups:With this object. You can do a lot of things including: **iteration**, **aggregation**, or **transformation**. So we could iterate like so:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Iteration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for patient, group in cdystonia_grouped:\n",
      "    print patient\n",
      "    print group\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A common data analysis procedure is the **split-apply-combine** operation, which groups subsets of data together, applies a function to each of the groups, then recombines them into a new data table.\n",
      "\n",
      "For example, we may want to aggregate our data with with some function.\n",
      "\n",
      "![split-apply-combine](http://f.cl.ly/items/0s0Z252j0X0c3k3P1M47/Screen%20Shot%202013-06-02%20at%203.04.04%20PM.png)\n",
      "\n",
      "<div align=\"right\">*(Source: \"Python for Data Analysis\", p.251)*</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Aggregation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can aggregate in Pandas using the `aggregate` (or `agg`, for short) method:mm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia_grouped.agg(np.mean).head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that the `treat` and `sex` variables are not included in the aggregation. Since it does not make sense to aggregate non-string variables, these columns are simply ignored by the method.\n",
      "\n",
      "Some aggregation functions are so common that Pandas has a convenience method for them, such as `mean`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia_grouped.mean().head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `add_prefix` and `add_suffix` methods can be used to give the columns of the resulting table labels that reflect the transformation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia_grouped.mean().add_suffix('_mean').head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The median of the `twstrs` variable\n",
      "cdystonia_grouped['twstrs'].quantile(0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we wish, we can easily aggregate according to multiple keys:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia.groupby(['week','site']).mean().head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Transform"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alternately, we can **transform** the data, using a function of our choice with the `transform` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "normalize = lambda x: (x - x.mean())/x.std()\n",
      "\n",
      "cdystonia_grouped.transform(normalize).head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is easy to do column selection within `groupby` operations, if we are only interested split-apply-combine operations on a subset of columns:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cdystonia_grouped['twstrs'].mean().head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you simply want to divide your DataFrame into chunks for later use, its easy to convert them into a dict so that they can be easily indexed out as needed:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunks = dict(list(cdystonia_grouped))\n",
      "chunks[4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default, `groupby` groups by row, but we can specify the `axis` argument to change this. For example, we can group our columns by type this way:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dict(list(cdystonia.groupby(cdystonia.dtypes, axis=1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Apply"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can generalize the split-apply-combine methodology by using `apply` function. This allows us to invoke any function we wish on a grouped dataset and recombine them into a DataFrame.\n",
      "\n",
      "The function below takes a DataFrame and a column name, sorts by the column, and takes the `n` largest values of that column. We can use this with `apply` to return the largest values from every group in a DataFrame in a single call. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def top(df, column, n=5):\n",
      "    return df.sort_index(by=column, ascending=False)[:n]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see this in action, consider the vessel transit segments dataset (which we merged with the vessel information to yield `segments_merged`). Say we wanted to return the 3 longest segments travelled by each ship:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top3segments = segments_m.groupby('mmsi').apply(top, column='seg_length', n=3)[['names', 'seg_length']]\n",
      "top3segments"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that additional arguments for the applied function can be passed via `apply` after the function name. It assumes that the DataFrame is the first argument."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top3segments.head(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classwork"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets use `apply` to calculate a meaningful baseball statistics, slugging percentage:\n",
      "\n",
      "$$SLG = \\frac{1B + (2 \\times 2B) + (3 \\times 3B) + (4 \\times HR)}{AB}$$\n",
      "\n",
      "And just for fun, we will format the resulting estimate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "slg = lambda x: (x['h']-x['X2b']-x['X3b']-x['hr'] + 2*x['X2b'] + 3*x['X3b'] + 4*x['hr'])/(x['ab']+1e-6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "to have 3 decimals of precision, append another apply to your function, i.e.\n",
      "`.apply(lambda x: '%.3f' % x)`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/resources.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section is based off of the following resources \n",
      "* [Group By: split-apply-combine](http://pandas.pydata.org/pandas-docs/version/0.13.1/groupby.html)\n",
      "* [GroupBy-fu: improvements in grouping and aggregating data in pandas](http://wesmckinney.com/blog/?p=125)\n",
      "* [Data Wrangling with Pandas](http://nbviewer.ipython.org/urls/gist.github.com/fonnesbeck/5850413/raw/3a9406c73365480bc58d5e75bc80f7962243ba17/2.+Data+Wrangling+with+Pandas.ipynb)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}